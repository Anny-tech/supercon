{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7ef57f7",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cb09bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba15251d",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36c2eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = '/home/digifort/Documents/Data_Management_F25/supercon/raw_data/primary.tsv'\n",
    "OUTPUT_DIR = '/home/digifort/Documents/Data_Management_F25/supercon/clean_data/'\n",
    "LOG_DIR = '/home/digifort/Documents/Data_Management_F25/supercon/log_files/'\n",
    "\n",
    "LOG_FILE = os.path.join(LOG_DIR, 'cleaning_log.txt')\n",
    "\n",
    "# Quality thresholds\n",
    "MIN_TC = 0.01  \n",
    "MAX_TC = 300   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb696c1e",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c6af8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleaningLogger:\n",
    "    \"\"\"Log all data cleaning operations for transparency\"\"\"\n",
    "    def __init__(self, log_file):\n",
    "        self.log_file = log_file\n",
    "        self.logs = []\n",
    "    \n",
    "    def log(self, message):\n",
    "        self.logs.append(message)\n",
    "        print(message)\n",
    "    \n",
    "    def save(self):\n",
    "        with open(self.log_file, 'w') as f:\n",
    "            f.write('\\n'.join(self.logs))\n",
    "        print(f\"\\nLog saved to {self.log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31988f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_formula_elements(formula):\n",
    "    \"\"\"\n",
    "    Extract elements from a chemical formula.\n",
    "    Handles formulas like 'Ba0.2La1.8Cu1O4-Y' \n",
    "    Returns set of element symbols\n",
    "    \"\"\"\n",
    "    if pd.isna(formula) or not isinstance(formula, str):\n",
    "        return set()\n",
    "    \n",
    "    # Remove common suffixes\n",
    "    formula = re.sub(r'-[A-Z]$', '', formula)  # Remove -Y, -Z suffixes\n",
    "    \n",
    "    # Pattern to match element symbols (Capital letter followed by optional lowercase)\n",
    "    element_pattern = r'[A-Z][a-z]?'\n",
    "    elements = set(re.findall(element_pattern, formula))\n",
    "    \n",
    "    # Filter out common non-element symbols\n",
    "    non_elements = {'Y', 'Z', 'X'}  # These are often used as variables\n",
    "    # Keep Y as Yttrium is a real element, but in context -Y means oxygen deficiency\n",
    "    \n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "254b48a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_formula(formula):\n",
    "    \"\"\"\n",
    "    Normalize chemical formula by:\n",
    "    - Converting to standard case\n",
    "    - Removing extra whitespace\n",
    "    - Flagging oxygen non-stoichiometry\n",
    "    \"\"\"\n",
    "    if pd.isna(formula) or not isinstance(formula, str):\n",
    "        return None, True  # Return None and flag as problematic\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    formula = formula.strip()\n",
    "    \n",
    "    # Check for oxygen non-stoichiometry markers\n",
    "    has_oxygen_var = bool(re.search(r'O\\d*-[XYZ]', formula))\n",
    "    \n",
    "    return formula, has_oxygen_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecc00934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tc_value(tc_str):\n",
    "    \"\"\"\n",
    "    Extract numerical Tc value from string.\n",
    "    Handles various formats and flags uncertainties.\n",
    "    \"\"\"\n",
    "    if pd.isna(tc_str):\n",
    "        return None, True\n",
    "    \n",
    "    # If already numeric\n",
    "    if isinstance(tc_str, (int, float)):\n",
    "        return float(tc_str), False\n",
    "    \n",
    "    # Convert to string and clean\n",
    "    tc_str = str(tc_str).strip()\n",
    "    \n",
    "    # Check for uncertainty markers\n",
    "    has_uncertainty = bool(re.search(r'[~<>≈±]', tc_str))\n",
    "    \n",
    "    # Extract first number\n",
    "    match = re.search(r'[-+]?\\d*\\.?\\d+', tc_str)\n",
    "    if match:\n",
    "        try:\n",
    "            value = float(match.group())\n",
    "            return value, has_uncertainty\n",
    "        except ValueError:\n",
    "            return None, True\n",
    "    \n",
    "    return None, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61352f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_tc(tc):\n",
    "    \"\"\"\n",
    "    Validate Tc value is physically reasonable.\n",
    "    Returns validation status and reason.\n",
    "    \"\"\"\n",
    "    if pd.isna(tc):\n",
    "        return 'missing', 'Tc value missing'\n",
    "    \n",
    "    if tc < MIN_TC:\n",
    "        return 'too_low', f'Tc below threshold ({MIN_TC}K)'\n",
    "    \n",
    "    if tc > MAX_TC:\n",
    "        return 'too_high', f'Tc above threshold ({MAX_TC}K)'\n",
    "    \n",
    "    return 'valid', 'Valid Tc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2ccfd2",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4f07c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, logger):\n",
    "    \"\"\"Load the raw TSV data\"\"\"\n",
    "    logger.log(\"=\" * 70)\n",
    "    logger.log(\"STEP 1: LOADING DATA\")\n",
    "    logger.log(\"=\" * 70)\n",
    "    \n",
    "    # The first 3 rows contain metadata/headers\n",
    "    # Row 0: column numbers\n",
    "    # Row 1: descriptive column names\n",
    "    # Row 2: short column codes\n",
    "    # Actual data starts from row 3\n",
    "    \n",
    "    df = pd.read_csv(file_path, sep='\\t', skiprows=3, encoding='utf-8')\n",
    "    \n",
    "    logger.log(f\"Loaded {len(df)} records from {file_path}\")\n",
    "    logger.log(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60c8d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_columns(df, logger):\n",
    "    \"\"\"Rename columns to standardized names\"\"\"\n",
    "    logger.log(\"\\n\" + \"=\" * 70)\n",
    "    logger.log(\"STEP 2: STANDARDIZING COLUMN NAMES\")\n",
    "    logger.log(\"=\" * 70)\n",
    "    \n",
    "    # Based on the header structure observed\n",
    "    column_mapping = {\n",
    "        df.columns[0]: 'data_number',\n",
    "        df.columns[1]: 'common_formula',\n",
    "        df.columns[2]: 'chemical_formula',\n",
    "        df.columns[3]: 'structure_name',\n",
    "        df.columns[4]: 'tc_unit',\n",
    "        df.columns[5]: 'tc_value',\n",
    "        df.columns[6]: 'journal_reference'\n",
    "    }\n",
    "    \n",
    "    df = df.rename(columns=column_mapping)\n",
    "    logger.log(f\"Standardized column names: {list(df.columns)}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d51fd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, logger):\n",
    "    \"\"\"Identify and flag missing values\"\"\"\n",
    "    logger.log(\"\\n\" + \"=\" * 70)\n",
    "    logger.log(\"STEP 3: HANDLING MISSING VALUES\")\n",
    "    logger.log(\"=\" * 70)\n",
    "    \n",
    "    # Count missing values per column\n",
    "    missing_counts = df.isnull().sum()\n",
    "    logger.log(\"\\nMissing value counts by column:\")\n",
    "    for col, count in missing_counts.items():\n",
    "        pct = 100 * count / len(df)\n",
    "        logger.log(f\"  {col}: {count} ({pct:.2f}%)\")\n",
    "    \n",
    "    # Flag rows with critical missing data\n",
    "    df['missing_formula'] = df['chemical_formula'].isnull()\n",
    "    df['missing_tc'] = df['tc_value'].isnull()\n",
    "    df['missing_critical'] = df['missing_formula'] | df['missing_tc']\n",
    "    \n",
    "    n_critical = df['missing_critical'].sum()\n",
    "    logger.log(f\"\\nRows with critical missing data (formula or Tc): {n_critical}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c36e57f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_formulas(df, logger):\n",
    "    \"\"\"Clean and normalize chemical formulas\"\"\"\n",
    "    logger.log(\"\\n\" + \"=\" * 70)\n",
    "    logger.log(\"STEP 4: CLEANING CHEMICAL FORMULAS\")\n",
    "    logger.log(\"=\" * 70)\n",
    "    \n",
    "    # Apply normalization\n",
    "    results = df['chemical_formula'].apply(normalize_formula)\n",
    "    df['formula_normalized'] = [r[0] for r in results]\n",
    "    df['has_oxygen_var'] = [r[1] for r in results]\n",
    "    \n",
    "    # Extract elements\n",
    "    df['elements'] = df['formula_normalized'].apply(parse_formula_elements)\n",
    "    df['n_elements'] = df['elements'].apply(len)\n",
    "    \n",
    "    logger.log(f\"Formulas with oxygen non-stoichiometry: {df['has_oxygen_var'].sum()}\")\n",
    "    logger.log(f\"\\nElement count distribution:\")\n",
    "    logger.log(df['n_elements'].value_counts().sort_index().to_string())\n",
    "    \n",
    "    # Identify most common elements\n",
    "    all_elements = []\n",
    "    for elem_set in df['elements'].dropna():\n",
    "        all_elements.extend(elem_set)\n",
    "    \n",
    "    element_counts = Counter(all_elements)\n",
    "    logger.log(f\"\\nTop 15 most common elements:\")\n",
    "    for elem, count in element_counts.most_common(15):\n",
    "        pct = 100 * count / len(df)\n",
    "        logger.log(f\"  {elem}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57624d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_tc_values(df, logger):\n",
    "    \"\"\"Clean and validate Tc values\"\"\"\n",
    "    logger.log(\"\\n\" + \"=\" * 70)\n",
    "    logger.log(\"STEP 5: CLEANING Tc VALUES\")\n",
    "    logger.log(\"=\" * 70)\n",
    "    \n",
    "    # Extract Tc values\n",
    "    results = df['tc_value'].apply(extract_tc_value)\n",
    "    df['tc_kelvin'] = [r[0] for r in results]\n",
    "    df['tc_has_uncertainty'] = [r[1] for r in results]\n",
    "    \n",
    "    # Validate Tc values\n",
    "    validation_results = df['tc_kelvin'].apply(validate_tc)\n",
    "    df['tc_validation'] = [r[0] for r in validation_results]\n",
    "    df['tc_validation_reason'] = [r[1] for r in validation_results]\n",
    "    \n",
    "    # Summary statistics\n",
    "    logger.log(\"\\nTc Validation Results:\")\n",
    "    logger.log(df['tc_validation'].value_counts().to_string())\n",
    "    \n",
    "    valid_tcs = df[df['tc_validation'] == 'valid']['tc_kelvin']\n",
    "    logger.log(f\"\\nValid Tc Statistics:\")\n",
    "    logger.log(f\"  Count: {len(valid_tcs)}\")\n",
    "    logger.log(f\"  Mean: {valid_tcs.mean():.2f} K\")\n",
    "    logger.log(f\"  Median: {valid_tcs.median():.2f} K\")\n",
    "    logger.log(f\"  Std: {valid_tcs.std():.2f} K\")\n",
    "    logger.log(f\"  Min: {valid_tcs.min():.2f} K\")\n",
    "    logger.log(f\"  Max: {valid_tcs.max():.2f} K\")\n",
    "    logger.log(f\"  25th percentile: {valid_tcs.quantile(0.25):.2f} K\")\n",
    "    logger.log(f\"  75th percentile: {valid_tcs.quantile(0.75):.2f} K\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d197c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_duplicates(df, logger):\n",
    "    \"\"\"Detect potential duplicate entries\"\"\"\n",
    "    logger.log(\"\\n\" + \"=\" * 70)\n",
    "    logger.log(\"STEP 6: DETECTING DUPLICATES\")\n",
    "    logger.log(\"=\" * 70)\n",
    "    \n",
    "    # Check for exact formula duplicates\n",
    "    formula_dups = df['formula_normalized'].duplicated(keep=False)\n",
    "    n_formula_dups = formula_dups.sum()\n",
    "    \n",
    "    logger.log(f\"Records with duplicate formulas: {n_formula_dups}\")\n",
    "    \n",
    "    # Check for exact formula + Tc duplicates\n",
    "    formula_tc_dups = df.duplicated(subset=['formula_normalized', 'tc_kelvin'], keep=False)\n",
    "    n_exact_dups = formula_tc_dups.sum()\n",
    "    \n",
    "    logger.log(f\"Records with duplicate formula + Tc: {n_exact_dups}\")\n",
    "    \n",
    "    df['is_duplicate_formula'] = formula_dups\n",
    "    df['is_duplicate_exact'] = formula_tc_dups\n",
    "    \n",
    "    # Show examples of duplicates\n",
    "    if n_formula_dups > 0:\n",
    "        logger.log(\"\\nExample duplicate formulas (first 5):\")\n",
    "        dup_formulas = df[formula_dups].groupby('formula_normalized').head(2)\n",
    "        for formula, group in dup_formulas.groupby('formula_normalized'):\n",
    "            if len(group) > 1:\n",
    "                logger.log(f\"\\n  Formula: {formula}\")\n",
    "                for _, row in group.iterrows():\n",
    "                    logger.log(f\"    Data #{row['data_number']}: Tc = {row['tc_kelvin']} K\")\n",
    "                break  # Just show first example\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f74468aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quality_tiers(df, logger):\n",
    "    \"\"\"Create quality tier classifications\"\"\"\n",
    "    logger.log(\"\\n\" + \"=\" * 70)\n",
    "    logger.log(\"STEP 7: CREATING QUALITY TIERS\")\n",
    "    logger.log(\"=\" * 70)\n",
    "    \n",
    "    # Define quality tiers\n",
    "    # Tier 1 (Strict): Valid Tc, no missing critical data, no oxygen var, not duplicate\n",
    "    tier1 = (\n",
    "        (df['tc_validation'] == 'valid') &\n",
    "        (~df['missing_critical']) &\n",
    "        (~df['has_oxygen_var']) &\n",
    "        (~df['is_duplicate_exact'])\n",
    "    )\n",
    "    \n",
    "    # Tier 2 (Standard): Valid Tc, no missing critical data\n",
    "    tier2 = (\n",
    "        (df['tc_validation'] == 'valid') &\n",
    "        (~df['missing_critical'])\n",
    "    )\n",
    "    \n",
    "    # Tier 3 (Inclusive): Has Tc value (even if questionable)\n",
    "    tier3 = df['tc_kelvin'].notna()\n",
    "    \n",
    "    df['quality_tier'] = 'excluded'\n",
    "    df.loc[tier3, 'quality_tier'] = 'tier3_inclusive'\n",
    "    df.loc[tier2, 'quality_tier'] = 'tier2_standard'\n",
    "    df.loc[tier1, 'quality_tier'] = 'tier1_strict'\n",
    "    \n",
    "    logger.log(\"\\nQuality Tier Distribution:\")\n",
    "    logger.log(df['quality_tier'].value_counts().to_string())\n",
    "    \n",
    "    # Calculate tier percentages\n",
    "    for tier in ['tier1_strict', 'tier2_standard', 'tier3_inclusive']:\n",
    "        count = (df['quality_tier'] == tier).sum()\n",
    "        pct = 100 * count / len(df)\n",
    "        logger.log(f\"  {tier}: {count} records ({pct:.1f}%)\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a35ede42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_metadata(df, logger):\n",
    "    \"\"\"Add useful metadata columns\"\"\"\n",
    "    logger.log(\"\\n\" + \"=\" * 70)\n",
    "    logger.log(\"STEP 8: ADDING METADATA\")\n",
    "    logger.log(\"=\" * 70)\n",
    "    \n",
    "    # Extract year from journal reference (if possible)\n",
    "    def extract_year(ref):\n",
    "        if pd.isna(ref):\n",
    "            return None\n",
    "        match = re.search(r'\\((\\d{4})\\)', str(ref))\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        match = re.search(r'(\\d{4})', str(ref))\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        return None\n",
    "    \n",
    "    df['publication_year'] = df['journal_reference'].apply(extract_year)\n",
    "    \n",
    "    year_counts = df['publication_year'].value_counts().sort_index()\n",
    "    logger.log(f\"\\nPublications by year: {len(year_counts)} unique years\")\n",
    "    if len(year_counts) > 0:\n",
    "        logger.log(f\"  Earliest: {year_counts.index.min()}\")\n",
    "        logger.log(f\"  Latest: {year_counts.index.max()}\")\n",
    "    \n",
    "    # Flag high-Tc materials (>77K, liquid nitrogen temperature)\n",
    "    df['is_high_tc'] = df['tc_kelvin'] > 77\n",
    "    \n",
    "    # Categorize by common superconductor families\n",
    "    def categorize_family(elements):\n",
    "        if pd.isna(elements) or len(elements) == 0:\n",
    "            return 'unknown'\n",
    "        \n",
    "        if 'Cu' in elements and 'O' in elements:\n",
    "            return 'cuprate'\n",
    "        elif 'Fe' in elements:\n",
    "            return 'iron_based'\n",
    "        elif 'Nb' in elements:\n",
    "            return 'niobium'\n",
    "        elif 'Hg' in elements:\n",
    "            return 'mercury_based'\n",
    "        else:\n",
    "            return 'other'\n",
    "    \n",
    "    df['material_family'] = df['elements'].apply(categorize_family)\n",
    "    \n",
    "    logger.log(\"\\nMaterial Family Distribution:\")\n",
    "    logger.log(df['material_family'].value_counts().to_string())\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a59e3bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cleaned_data(df, output_dir, logger):\n",
    "    \"\"\"Save cleaned data in multiple formats\"\"\"\n",
    "    logger.log(\"\\n\" + \"=\" * 70)\n",
    "    logger.log(\"STEP 9: SAVING CLEANED DATA\")\n",
    "    logger.log(\"=\" * 70)\n",
    "    \n",
    "    import os\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save full dataset with all flags\n",
    "    full_path = os.path.join(output_dir, 'superconductors_full_cleaned.csv')\n",
    "    df.to_csv(full_path, index=False)\n",
    "    logger.log(f\"Saved full dataset: {full_path}\")\n",
    "    \n",
    "    # Save tier 1 (strict quality)\n",
    "    tier1_df = df[df['quality_tier'] == 'tier1_strict'].copy()\n",
    "    tier1_path = os.path.join(output_dir, 'superconductors_tier1_strict.csv')\n",
    "    tier1_df.to_csv(tier1_path, index=False)\n",
    "    logger.log(f\"Saved Tier 1 dataset: {tier1_path} ({len(tier1_df)} records)\")\n",
    "    \n",
    "    # Save tier 2 (standard quality)\n",
    "    tier2_df = df[df['quality_tier'].isin(['tier1_strict', 'tier2_standard'])].copy()\n",
    "    tier2_path = os.path.join(output_dir, 'superconductors_tier2_standard.csv')\n",
    "    tier2_df.to_csv(tier2_path, index=False)\n",
    "    logger.log(f\"Saved Tier 2 dataset: {tier2_path} ({len(tier2_df)} records)\")\n",
    "    \n",
    "    # Create summary statistics file\n",
    "    summary_path = os.path.join(output_dir, 'data_summary.txt')\n",
    "    with open(summary_path, 'w') as f:\n",
    "        f.write(\"SUPERCONDUCTOR DATA CLEANING SUMMARY\\n\")\n",
    "        f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "        f.write(f\"Total records: {len(df)}\\n\")\n",
    "        f.write(f\"Tier 1 (strict): {len(tier1_df)} ({100*len(tier1_df)/len(df):.1f}%)\\n\")\n",
    "        f.write(f\"Tier 2 (standard): {len(tier2_df)} ({100*len(tier2_df)/len(df):.1f}%)\\n\\n\")\n",
    "        \n",
    "        f.write(\"Tc Statistics (valid records):\\n\")\n",
    "        valid_tc = df[df['tc_validation'] == 'valid']['tc_kelvin']\n",
    "        f.write(f\"  Mean: {valid_tc.mean():.2f} K\\n\")\n",
    "        f.write(f\"  Median: {valid_tc.median():.2f} K\\n\")\n",
    "        f.write(f\"  Range: {valid_tc.min():.2f} - {valid_tc.max():.2f} K\\n\\n\")\n",
    "        \n",
    "        f.write(\"Most common elements:\\n\")\n",
    "        all_elements = []\n",
    "        for elem_set in df['elements'].dropna():\n",
    "            all_elements.extend(elem_set)\n",
    "        for elem, count in Counter(all_elements).most_common(10):\n",
    "            f.write(f\"  {elem}: {count}\\n\")\n",
    "    \n",
    "    logger.log(f\"Saved summary: {summary_path}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "874c9049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quality_report(df, logger):\n",
    "    \"\"\"Generate detailed quality report\"\"\"\n",
    "    logger.log(\"\\n\" + \"=\" * 70)\n",
    "    logger.log(\"STEP 10: QUALITY REPORT\")\n",
    "    logger.log(\"=\" * 70)\n",
    "    \n",
    "    logger.log(\"\\nDATA QUALITY SUMMARY:\")\n",
    "    logger.log(f\"  Total records: {len(df)}\")\n",
    "    logger.log(f\"  Records with valid Tc: {(df['tc_validation'] == 'valid').sum()}\")\n",
    "    logger.log(f\"  Records with oxygen variability: {df['has_oxygen_var'].sum()}\")\n",
    "    logger.log(f\"  Duplicate formulas: {df['is_duplicate_formula'].sum()}\")\n",
    "    logger.log(f\"  High-Tc materials (>77K): {df['is_high_tc'].sum()}\")\n",
    "    \n",
    "    logger.log(\"\\nRECOMMENDATIONS:\")\n",
    "    logger.log(\"  1. Use 'tier1_strict' for high-quality analyses\")\n",
    "    logger.log(\"  2. Use 'tier2_standard' for broader coverage\")\n",
    "    logger.log(\"  3. Filter by 'has_oxygen_var' when precise stoichiometry matters\")\n",
    "    logger.log(\"  4. Check 'is_duplicate_formula' flag for multiple measurements\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d88e5388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run the complete data cleaning pipeline\"\"\"\n",
    "    \n",
    "    # Initialize logger\n",
    "    logger = CleaningLogger(LOG_FILE)\n",
    "    logger.log(\"SUPERCONDUCTOR DATA CLEANING PIPELINE\")\n",
    "    logger.log(f\"Date: {pd.Timestamp.now()}\")\n",
    "    logger.log(f\"Input: {INPUT_FILE}\")\n",
    "    \n",
    "    try:\n",
    "        # Execute pipeline steps\n",
    "        df = load_data(INPUT_FILE, logger)\n",
    "        df = standardize_columns(df, logger)\n",
    "        df = handle_missing_values(df, logger)\n",
    "        df = clean_formulas(df, logger)\n",
    "        df = clean_tc_values(df, logger)\n",
    "        df = detect_duplicates(df, logger)\n",
    "        df = create_quality_tiers(df, logger)\n",
    "        df = add_metadata(df, logger)\n",
    "        df = save_cleaned_data(df, OUTPUT_DIR, logger)\n",
    "        generate_quality_report(df, logger)\n",
    "        \n",
    "        logger.log(\"\\n\" + \"=\" * 70)\n",
    "        logger.log(\"CLEANING PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "        logger.log(\"=\" * 70)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.log(f\"\\nERROR: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.log(traceback.format_exc())\n",
    "    \n",
    "    finally:\n",
    "        logger.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "684116a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUPERCONDUCTOR DATA CLEANING PIPELINE\n",
      "Date: 2025-12-08 00:06:18.312913\n",
      "Input: /home/digifort/Documents/Data_Management_F25/supercon/raw_data/primary.tsv\n",
      "======================================================================\n",
      "STEP 1: LOADING DATA\n",
      "======================================================================\n",
      "Loaded 26357 records from /home/digifort/Documents/Data_Management_F25/supercon/raw_data/primary.tsv\n",
      "Columns: ['2', '(Ba,La)2CuO4', 'Ba0.2La1.8Cu1O4-Y', 'T', 'Unnamed: 4', '29', 'Jpn.J.Appl.Phys., 26(1987)L223']\n",
      "\n",
      "======================================================================\n",
      "STEP 2: STANDARDIZING COLUMN NAMES\n",
      "======================================================================\n",
      "Standardized column names: ['data_number', 'common_formula', 'chemical_formula', 'structure_name', 'tc_unit', 'tc_value', 'journal_reference']\n",
      "\n",
      "======================================================================\n",
      "STEP 3: HANDLING MISSING VALUES\n",
      "======================================================================\n",
      "\n",
      "Missing value counts by column:\n",
      "  data_number: 0 (0.00%)\n",
      "  common_formula: 1290 (4.89%)\n",
      "  chemical_formula: 0 (0.00%)\n",
      "  structure_name: 6764 (25.66%)\n",
      "  tc_unit: 6346 (24.08%)\n",
      "  tc_value: 0 (0.00%)\n",
      "  journal_reference: 75 (0.28%)\n",
      "\n",
      "Rows with critical missing data (formula or Tc): 0\n",
      "\n",
      "======================================================================\n",
      "STEP 4: CLEANING CHEMICAL FORMULAS\n",
      "======================================================================\n",
      "Formulas with oxygen non-stoichiometry: 777\n",
      "\n",
      "Element count distribution:\n",
      "1      363\n",
      "2     4354\n",
      "3     5354\n",
      "4     5714\n",
      "5     6251\n",
      "6     2985\n",
      "7     1087\n",
      "8      215\n",
      "9       22\n",
      "10       1\n",
      "11      11\n",
      "\n",
      "Top 15 most common elements:\n",
      "  O: 12548 (47.6%)\n",
      "  Cu: 12257 (46.5%)\n",
      "  Ba: 7699 (29.2%)\n",
      "  Sr: 5528 (21.0%)\n",
      "  Y: 4831 (18.3%)\n",
      "  Ca: 4671 (17.7%)\n",
      "  La: 4067 (15.4%)\n",
      "  Fe: 2945 (11.2%)\n",
      "  Bi: 2906 (11.0%)\n",
      "  As: 1959 (7.4%)\n",
      "  Nb: 1718 (6.5%)\n",
      "  C: 1649 (6.3%)\n",
      "  B: 1531 (5.8%)\n",
      "  Ce: 1459 (5.5%)\n",
      "  Ni: 1432 (5.4%)\n",
      "\n",
      "======================================================================\n",
      "STEP 5: CLEANING Tc VALUES\n",
      "======================================================================\n",
      "\n",
      "Tc Validation Results:\n",
      "valid       26248\n",
      "too_low       108\n",
      "too_high        1\n",
      "\n",
      "Valid Tc Statistics:\n",
      "  Count: 26248\n",
      "  Mean: 32.78 K\n",
      "  Median: 17.30 K\n",
      "  Std: 34.40 K\n",
      "  Min: 0.01 K\n",
      "  Max: 294.00 K\n",
      "  25th percentile: 4.60 K\n",
      "  75th percentile: 58.90 K\n",
      "\n",
      "======================================================================\n",
      "STEP 6: DETECTING DUPLICATES\n",
      "======================================================================\n",
      "Records with duplicate formulas: 11663\n",
      "Records with duplicate formula + Tc: 4045\n",
      "\n",
      "Example duplicate formulas (first 5):\n",
      "\n",
      "  Formula: Ag0.81Sn1.19Se2\n",
      "    Data #109839: Tc = 6.47 K\n",
      "    Data #150303: Tc = 6.36 K\n",
      "\n",
      "======================================================================\n",
      "STEP 7: CREATING QUALITY TIERS\n",
      "======================================================================\n",
      "\n",
      "Quality Tier Distribution:\n",
      "tier1_strict       21580\n",
      "tier2_standard      4668\n",
      "tier3_inclusive      109\n",
      "  tier1_strict: 21580 records (81.9%)\n",
      "  tier2_standard: 4668 records (17.7%)\n",
      "  tier3_inclusive: 109 records (0.4%)\n",
      "\n",
      "======================================================================\n",
      "STEP 8: ADDING METADATA\n",
      "======================================================================\n",
      "\n",
      "Publications by year: 116 unique years\n",
      "  Earliest: 105.0\n",
      "  Latest: 2995.0\n",
      "\n",
      "Material Family Distribution:\n",
      "other            11120\n",
      "cuprate          10803\n",
      "iron_based        2513\n",
      "niobium           1656\n",
      "mercury_based      265\n",
      "\n",
      "======================================================================\n",
      "STEP 9: SAVING CLEANED DATA\n",
      "======================================================================\n",
      "Saved full dataset: /home/digifort/Documents/Data_Management_F25/supercon/clean_data/superconductors_full_cleaned.csv\n",
      "Saved Tier 1 dataset: /home/digifort/Documents/Data_Management_F25/supercon/clean_data/superconductors_tier1_strict.csv (21580 records)\n",
      "Saved Tier 2 dataset: /home/digifort/Documents/Data_Management_F25/supercon/clean_data/superconductors_tier2_standard.csv (26248 records)\n",
      "Saved summary: /home/digifort/Documents/Data_Management_F25/supercon/clean_data/data_summary.txt\n",
      "\n",
      "======================================================================\n",
      "STEP 10: QUALITY REPORT\n",
      "======================================================================\n",
      "\n",
      "DATA QUALITY SUMMARY:\n",
      "  Total records: 26357\n",
      "  Records with valid Tc: 26248\n",
      "  Records with oxygen variability: 777\n",
      "  Duplicate formulas: 11663\n",
      "  High-Tc materials (>77K): 4707\n",
      "\n",
      "RECOMMENDATIONS:\n",
      "  1. Use 'tier1_strict' for high-quality analyses\n",
      "  2. Use 'tier2_standard' for broader coverage\n",
      "  3. Filter by 'has_oxygen_var' when precise stoichiometry matters\n",
      "  4. Check 'is_duplicate_formula' flag for multiple measurements\n",
      "\n",
      "======================================================================\n",
      "CLEANING PIPELINE COMPLETED SUCCESSFULLY\n",
      "======================================================================\n",
      "\n",
      "Log saved to /home/digifort/Documents/Data_Management_F25/supercon/log_files/cleaning_log.txt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecd988d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bi2te3_env)",
   "language": "python",
   "name": "bi2te3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

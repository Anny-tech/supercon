{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5683baef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nComprehensive Analysis for Superconductor Dashboard\\n====================================================\\nAuthor: Ankita Biswas\\nProject: Public Dashboard for Superconductors\\nDate: December 2025\\n\\nThis script performs:\\n1. Dashboard-focused EDA (following project proposal)\\n2. Feature importance analysis\\n3. PCA for dimensionality reduction\\n4. Data preparation for regression modeling\\n\\nInput: Feature-engineered dataset with ~164 columns including matminer features\\nOutput: Dashboard visualizations, processed datasets, and analysis summaries\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Comprehensive Analysis for Superconductor Dashboard\n",
    "====================================================\n",
    "Author: Ankita Biswas\n",
    "Project: Public Dashboard for Superconductors\n",
    "Date: December 2025\n",
    "\n",
    "This script performs:\n",
    "1. Dashboard-focused EDA (following project proposal)\n",
    "2. Feature importance analysis\n",
    "3. PCA for dimensionality reduction\n",
    "4. Data preparation for regression modeling\n",
    "\n",
    "Input: Feature-engineered dataset with ~164 columns including matminer features\n",
    "Output: Dashboard visualizations, processed datasets, and analysis summaries\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "810bb05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from scipy.stats import spearmanr\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import ast\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 600\n",
    "plt.rcParams['savefig.dpi'] = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa906ea2",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28c8c647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUPERCONDUCTOR COMPREHENSIVE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Input: /home/digifort/Documents/Data_Management_F25/supercon/feature_engineered_data/superconductors_with_features.csv\n",
      "Output: /home/digifort/Documents/Data_Management_F25/supercon/analysis_results/\n"
     ]
    }
   ],
   "source": [
    "INPUT_FILE = '/home/digifort/Documents/Data_Management_F25/supercon/feature_engineered_data/superconductors_with_features.csv'\n",
    "OUTPUT_DIR = '/home/digifort/Documents/Data_Management_F25/supercon/analysis_results/'\n",
    "FIGURES_DIR = os.path.join(OUTPUT_DIR, 'figures/')\n",
    "TABLES_DIR = os.path.join(OUTPUT_DIR, 'tables/')\n",
    "MODELS_DIR = os.path.join(OUTPUT_DIR, 'models/')\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [OUTPUT_DIR, FIGURES_DIR, TABLES_DIR, MODELS_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Analysis parameters\n",
    "CORRELATION_THRESHOLD = 0.6\n",
    "VARIANCE_THRESHOLD = 0.01\n",
    "N_TOP_FEATURES = 5\n",
    "N_PCA_COMPONENTS = 10\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SUPERCONDUCTOR COMPREHENSIVE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nInput: {INPUT_FILE}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba8f6ba",
   "metadata": {},
   "source": [
    "### Data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d9e6b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING DATA\n",
      "================================================================================\n",
      "\n",
      "Dataset shape: (15845, 164)\n",
      "  Records: 15,845\n",
      "  Columns: 164\n",
      "\n",
      "Column types:\n",
      "  Metadata: 6\n",
      "  Categorical: 4\n",
      "  Binary flags: 4\n",
      "  Numerical features: 145\n",
      "  Target: tc_kelvin\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"  Records: {df.shape[0]:,}\")\n",
    "print(f\"  Columns: {df.shape[1]}\")\n",
    "\n",
    "# Parse elements column if it's a string\n",
    "if 'elements' in df.columns and df['elements'].dtype == 'object':\n",
    "    df['elements'] = df['elements'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Identify column types\n",
    "metadata_cols = [\n",
    "    'data_number', 'chemical_formula', 'formula_normalized', 'formula_clean',\n",
    "    'elements', 'composition'\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    'quality_tier', 'material_family', 'category_detailed', 'tc_validation'\n",
    "]\n",
    "\n",
    "binary_cols = [\n",
    "    'has_oxygen_var', 'is_duplicate_formula', 'is_high_tc', 'compound possible'\n",
    "]\n",
    "\n",
    "target_col = 'tc_kelvin'\n",
    "\n",
    "# Numerical feature columns (matminer features)\n",
    "exclude_cols = metadata_cols + categorical_cols + binary_cols + [target_col, 'tc_std', 'n_measurements', 'publication_year', 'n_elements']\n",
    "numerical_features = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"\\nColumn types:\")\n",
    "print(f\"  Metadata: {len([c for c in metadata_cols if c in df.columns])}\")\n",
    "print(f\"  Categorical: {len([c for c in categorical_cols if c in df.columns])}\")\n",
    "print(f\"  Binary flags: {len([c for c in binary_cols if c in df.columns])}\")\n",
    "print(f\"  Numerical features: {len(numerical_features)}\")\n",
    "print(f\"  Target: {target_col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8746a320",
   "metadata": {},
   "source": [
    "### Dashboard analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7b6e4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 1: DASHBOARD ANALYSES\n",
      "================================================================================\n",
      "\n",
      "[1/5] Distributions of Tc...\n",
      "\n",
      "  Tc Summary Statistics:\n",
      "    Count: 15,845\n",
      "    Mean: 29.87 K\n",
      "    Median: 14.40 K\n",
      "    Std: 32.82 K\n",
      "    Min: 0.01 K\n",
      "    Q25: 4.15 K\n",
      "    Q75: 50.00 K\n",
      "    Max: 294.00 K\n",
      "    Skewness: 1.16 K\n",
      "    Kurtosis: 0.67 K\n",
      "Saved: 01_tc_distributions.png\n",
      "Saved: 01b_tc_by_category.png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 1: DASHBOARD ANALYSES\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n[1/5] Distributions of Tc...\")\n",
    "\n",
    "tc = df[target_col].dropna()\n",
    "\n",
    "tc_stats = {\n",
    "    'Count': len(tc),\n",
    "    'Mean': tc.mean(),\n",
    "    'Median': tc.median(),\n",
    "    'Std': tc.std(),\n",
    "    'Min': tc.min(),\n",
    "    'Q25': tc.quantile(0.25),\n",
    "    'Q75': tc.quantile(0.75),\n",
    "    'Max': tc.max(),\n",
    "    'Skewness': tc.skew(),\n",
    "    'Kurtosis': tc.kurtosis()\n",
    "}\n",
    "\n",
    "print(\"\\n  Tc Summary Statistics:\")\n",
    "for key, value in tc_stats.items():\n",
    "    if key == 'Count':\n",
    "        print(f\"    {key}: {value:,.0f}\")\n",
    "    else:\n",
    "        print(f\"    {key}: {value:.2f} K\")\n",
    "\n",
    "pd.DataFrame([tc_stats]).to_csv(os.path.join(TABLES_DIR, 'tc_statistics.csv'), index=False)\n",
    "\n",
    "# Plot distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Histogram\n",
    "axes[0, 0].hist(tc, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0, 0].axvline(tc.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {tc.mean():.1f}K')\n",
    "axes[0, 0].axvline(tc.median(), color='orange', linestyle='--', linewidth=2, label=f'Median: {tc.median():.1f}K')\n",
    "axes[0, 0].axvline(77, color='green', linestyle=':', linewidth=2, label='LN₂ (77K)')\n",
    "axes[0, 0].set_xlabel('Critical Temperature (K)', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_title('Distribution of Tc', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=9)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Log-scale\n",
    "axes[0, 1].hist(np.log10(tc + 1), bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[0, 1].set_xlabel('log₁₀(Tc + 1)', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_title('Log-scale Distribution', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# CDF\n",
    "sorted_tc = np.sort(tc)\n",
    "cumulative = np.arange(1, len(sorted_tc) + 1) / len(sorted_tc)\n",
    "axes[1, 0].plot(sorted_tc, cumulative, linewidth=2, color='purple')\n",
    "axes[1, 0].axhline(0.5, color='red', linestyle='--', alpha=0.5)\n",
    "axes[1, 0].axhline(0.95, color='orange', linestyle='--', alpha=0.5)\n",
    "axes[1, 0].set_xlabel('Critical Temperature (K)', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Cumulative Probability', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_title('Cumulative Distribution', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1, 1].boxplot(tc, vert=True, widths=0.5, patch_artist=True,\n",
    "                   boxprops=dict(facecolor='lightblue'))\n",
    "axes[1, 1].set_ylabel('Critical Temperature (K)', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_title('Tc Box Plot', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, '01_tc_distributions.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: 01_tc_distributions.png\")\n",
    "\n",
    "# Tc by material family\n",
    "if 'category_detailed' in df.columns:\n",
    "    family_stats = df.groupby('category_detailed')[target_col].agg([\n",
    "        'count', 'mean', 'median', 'std', 'min', 'max'\n",
    "    ]).round(2).sort_values('mean', ascending=False)\n",
    "    family_stats.to_csv(os.path.join(TABLES_DIR, 'tc_by_category.csv'))\n",
    "    \n",
    "    # Plot top 10 categories\n",
    "    top_cats = family_stats.head(10).index\n",
    "    data_to_plot = [df[df['category_detailed'] == cat][target_col].dropna() for cat in top_cats]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    bp = ax.boxplot(data_to_plot, labels=top_cats, patch_artist=True)\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(top_cats)))\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    ax.set_ylabel('Tc (K)', fontsize=11, fontweight='bold')\n",
    "    ax.set_title('Tc by Material Category (Top 10)', fontsize=13, fontweight='bold')\n",
    "    ax.axhline(77, color='red', linestyle='--', alpha=0.5)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, '01b_tc_by_category.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"Saved: 01b_tc_by_category.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5192a0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/5] Element prevalence and Tc by element...\n",
      "  Total unique elements: 90\n",
      "Saved: 02_element_prevalence.png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[2/5] Element prevalence and Tc by element...\")\n",
    "\n",
    "all_elements = []\n",
    "for elem_set in df['elements'].dropna():\n",
    "    if isinstance(elem_set, set):\n",
    "        all_elements.extend(elem_set)\n",
    "\n",
    "element_counts = Counter(all_elements)\n",
    "print(f\"  Total unique elements: {len(element_counts)}\")\n",
    "\n",
    "# Top 20 elements\n",
    "top_elements = element_counts.most_common(20)\n",
    "elem_names = [e[0] for e in top_elements]\n",
    "\n",
    "# Save frequencies\n",
    "elem_freq_df = pd.DataFrame(element_counts.items(), columns=['element', 'count'])\n",
    "elem_freq_df['percentage'] = 100 * elem_freq_df['count'] / len(df)\n",
    "elem_freq_df = elem_freq_df.sort_values('count', ascending=False)\n",
    "elem_freq_df.to_csv(os.path.join(TABLES_DIR, 'element_frequencies.csv'), index=False)\n",
    "\n",
    "# Tc by element\n",
    "element_tc_stats = []\n",
    "for element in elem_names:\n",
    "    mask = df['elements'].apply(lambda x: element in x if isinstance(x, set) else False)\n",
    "    tc_with = df[mask][target_col].dropna()\n",
    "    \n",
    "    if len(tc_with) > 0:\n",
    "        element_tc_stats.append({\n",
    "            'element': element,\n",
    "            'count': len(tc_with),\n",
    "            'mean_tc': tc_with.mean(),\n",
    "            'median_tc': tc_with.median(),\n",
    "            'std_tc': tc_with.std()\n",
    "        })\n",
    "\n",
    "elem_tc_df = pd.DataFrame(element_tc_stats).sort_values('mean_tc', ascending=False)\n",
    "elem_tc_df.to_csv(os.path.join(TABLES_DIR, 'tc_by_element.csv'), index=False)\n",
    "\n",
    "# Plot element prevalence\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "elem_counts_vals = [e[1] for e in top_elements]\n",
    "ax.barh(range(20), elem_counts_vals[::-1], color='steelblue', edgecolor='black')\n",
    "ax.set_yticks(range(20))\n",
    "ax.set_yticklabels(elem_names[::-1], fontsize=10)\n",
    "ax.set_xlabel('Number of Materials', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Top 5 Most Common Elements', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, '02_element_prevalence.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: 02_element_prevalence.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c7905e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/5] Feature-outcome relationships...\n",
      "  Feature matrix: (15845, 145)\n",
      "  Computing correlations with Tc...\n",
      "\n",
      "  Top 10 features by Pearson correlation:\n",
      "                             feature  pearson_corr  spearman_corr\n",
      "      MagpieData avg_dev GSvolume_pa      0.678444       0.724032\n",
      "                      max ionic char      0.660387       0.699689\n",
      "  MagpieData range Electronegativity      0.651378       0.699889\n",
      "        MagpieData range GSvolume_pa      0.635451       0.698628\n",
      "                      avg ionic char      0.625714       0.664552\n",
      "                              0-norm      0.617598       0.688934\n",
      "MagpieData maximum Electronegativity      0.612725       0.648793\n",
      "     MagpieData range CovalentRadius      0.608645       0.691893\n",
      "      MagpieData maximum GSvolume_pa      0.606707       0.649574\n",
      "MagpieData avg_dev Electronegativity      0.601974       0.648012\n",
      "Saved: 03_feature_correlations.png\n",
      "Saved: 03b_scatter_plots.png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[3/5] Feature-outcome relationships...\")\n",
    "\n",
    "# Prepare feature matrix\n",
    "X = df[numerical_features].copy()\n",
    "y = df[target_col].copy()\n",
    "\n",
    "# Remove rows with missing target\n",
    "mask = y.notna()\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# Handle missing values in features\n",
    "X = X.dropna(axis=1, how='all')\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# Remove low variance features\n",
    "low_var_cols = X.columns[X.std() < VARIANCE_THRESHOLD]\n",
    "if len(low_var_cols) > 0:\n",
    "    print(f\"  Removing {len(low_var_cols)} low-variance features\")\n",
    "    X = X.drop(columns=low_var_cols)\n",
    "\n",
    "print(f\"  Feature matrix: {X.shape}\")\n",
    "\n",
    "# Correlation with Tc\n",
    "print(\"  Computing correlations with Tc...\")\n",
    "pearson_corr = X.corrwith(y).abs().sort_values(ascending=False)\n",
    "spearman_corr = pd.Series({col: abs(spearmanr(X[col], y)[0]) for col in X.columns}).sort_values(ascending=False)\n",
    "\n",
    "corr_results = pd.DataFrame({\n",
    "    'feature': pearson_corr.index,\n",
    "    'pearson_corr': pearson_corr.values,\n",
    "    'spearman_corr': [spearman_corr[f] for f in pearson_corr.index]\n",
    "})\n",
    "corr_results.to_csv(os.path.join(TABLES_DIR, 'feature_correlations.csv'), index=False)\n",
    "\n",
    "print(f\"\\n  Top 10 features by Pearson correlation:\")\n",
    "print(corr_results.head(10).to_string(index=False))\n",
    "\n",
    "# Plot correlations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].barh(range(5), pearson_corr.head(5).values[::-1], color='forestgreen')\n",
    "axes[0].set_yticks(range(5))\n",
    "axes[0].set_yticklabels(pearson_corr.head(5).index[::-1], fontsize=8)\n",
    "axes[0].set_xlabel('|Pearson Correlation|', fontsize=10, fontweight='bold')\n",
    "axes[0].set_title('Top 5 Features (Pearson)', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "axes[1].barh(range(5), spearman_corr.head(5).values[::-1], color='orange')\n",
    "axes[1].set_yticks(range(5))\n",
    "axes[1].set_yticklabels(spearman_corr.head(5).index[::-1], fontsize=8)\n",
    "axes[1].set_xlabel('|Spearman Correlation|', fontsize=10, fontweight='bold')\n",
    "axes[1].set_title('Top 5 Features (Spearman)', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, '03_feature_correlations.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: 03_feature_correlations.png\")\n",
    "\n",
    "# Scatter plots for top 6 features\n",
    "top_6_features = pearson_corr.head(6).index.tolist()\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(top_6_features):\n",
    "    x_data = X[feature]\n",
    "    y_data = y.loc[x_data.index]\n",
    "    \n",
    "    axes[i].scatter(x_data, y_data, alpha=0.3, s=15, edgecolors='k', linewidth=0.3)\n",
    "    \n",
    "    # Trend line\n",
    "    z = np.polyfit(x_data, y_data, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(x_data.min(), x_data.max(), 100)\n",
    "    axes[i].plot(x_line, p(x_line), \"r--\", linewidth=2)\n",
    "    \n",
    "    axes[i].set_xlabel(feature, fontsize=9, fontweight='bold')\n",
    "    axes[i].set_ylabel('Tc (K)', fontsize=9, fontweight='bold')\n",
    "    axes[i].set_title(f'r = {pearson_corr[feature]:.3f}', fontsize=10, fontweight='bold')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, '03b_scatter_plots.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: 03b_scatter_plots.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06a9afe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/5] Feature selection...\n",
      "  Method 1: F-test...\n",
      "  Method 2: Mutual Information...\n",
      "  Method 3: Random Forest...\n",
      "\n",
      "  Top 15 features by composite score:\n",
      "                             feature  composite_score\n",
      "      MagpieData avg_dev GSvolume_pa         0.798836\n",
      "                      max ionic char         0.614800\n",
      "  MagpieData range Electronegativity         0.604287\n",
      "        MagpieData range GSvolume_pa         0.594594\n",
      "     MagpieData range CovalentRadius         0.552062\n",
      "                      avg ionic char         0.518350\n",
      "MagpieData avg_dev Electronegativity         0.482451\n",
      "      MagpieData maximum GSvolume_pa         0.472654\n",
      "MagpieData maximum Electronegativity         0.468537\n",
      "           MagpieData mean NUnfilled         0.454620\n",
      "    MagpieData range MendeleevNumber         0.450056\n",
      "   MagpieData range SpaceGroupNumber         0.433247\n",
      " MagpieData avg_dev SpaceGroupNumber         0.424132\n",
      "   MagpieData minimum CovalentRadius         0.415509\n",
      "   MagpieData avg_dev CovalentRadius         0.407074\n",
      "Saved: 04_feature_importance.png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[5/5] Feature selection...\")\n",
    "\n",
    "# F-test\n",
    "print(\"  Method 1: F-test...\")\n",
    "selector_f = SelectKBest(score_func=f_regression, k=min(N_TOP_FEATURES, X.shape[1]))\n",
    "selector_f.fit(X, y)\n",
    "f_scores = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'f_score': selector_f.scores_\n",
    "}).sort_values('f_score', ascending=False)\n",
    "\n",
    "# Mutual Information\n",
    "print(\"  Method 2: Mutual Information...\")\n",
    "mi_scores = mutual_info_regression(X, y, random_state=42)\n",
    "mi_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'mi_score': mi_scores\n",
    "}).sort_values('mi_score', ascending=False)\n",
    "\n",
    "# Random Forest\n",
    "print(\"  Method 3: Random Forest...\")\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf.fit(X, y)\n",
    "rf_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'rf_importance': rf.feature_importances_\n",
    "}).sort_values('rf_importance', ascending=False)\n",
    "\n",
    "# Combine methods\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler_norm = MinMaxScaler()\n",
    "\n",
    "feature_importance = X.columns.to_frame(name='feature')\n",
    "feature_importance = feature_importance.merge(f_scores, on='feature')\n",
    "feature_importance = feature_importance.merge(mi_df, on='feature')\n",
    "feature_importance = feature_importance.merge(rf_importance, on='feature')\n",
    "\n",
    "feature_importance['f_score_norm'] = scaler_norm.fit_transform(feature_importance[['f_score']])\n",
    "feature_importance['mi_score_norm'] = scaler_norm.fit_transform(feature_importance[['mi_score']])\n",
    "feature_importance['rf_importance_norm'] = feature_importance['rf_importance']\n",
    "\n",
    "feature_importance['composite_score'] = (\n",
    "    feature_importance['f_score_norm'] + \n",
    "    feature_importance['mi_score_norm'] + \n",
    "    feature_importance['rf_importance_norm']\n",
    ") / 3\n",
    "\n",
    "feature_importance = feature_importance.sort_values('composite_score', ascending=False)\n",
    "feature_importance.to_csv(os.path.join(TABLES_DIR, 'feature_importance.csv'), index=False)\n",
    "\n",
    "print(f\"\\n  Top 15 features by composite score:\")\n",
    "print(feature_importance.head(15)[['feature', 'composite_score']].to_string(index=False))\n",
    "\n",
    "# Plot feature importance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "axes[0, 0].barh(range(15), f_scores.head(15)['f_score'].values[::-1], color='steelblue')\n",
    "axes[0, 0].set_yticks(range(15))\n",
    "axes[0, 0].set_yticklabels(f_scores.head(15)['feature'].values[::-1], fontsize=8)\n",
    "axes[0, 0].set_title('F-test', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "axes[0, 1].barh(range(15), mi_df.head(15)['mi_score'].values[::-1], color='coral')\n",
    "axes[0, 1].set_yticks(range(15))\n",
    "axes[0, 1].set_yticklabels(mi_df.head(15)['feature'].values[::-1], fontsize=8)\n",
    "axes[0, 1].set_title('Mutual Information', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "axes[1, 0].barh(range(15), rf_importance.head(15)['rf_importance'].values[::-1], color='forestgreen')\n",
    "axes[1, 0].set_yticks(range(15))\n",
    "axes[1, 0].set_yticklabels(rf_importance.head(15)['feature'].values[::-1], fontsize=8)\n",
    "axes[1, 0].set_title('Random Forest', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "axes[1, 1].barh(range(15), feature_importance.head(15)['composite_score'].values[::-1], color='purple')\n",
    "axes[1, 1].set_yticks(range(15))\n",
    "axes[1, 1].set_yticklabels(feature_importance.head(15)['feature'].values[::-1], fontsize=8)\n",
    "axes[1, 1].set_title('Composite Score', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, '04_feature_importance.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: 04_feature_importance.png\")\n",
    "\n",
    "# Select top features\n",
    "top_features = feature_importance.head(N_TOP_FEATURES)['feature'].tolist()\n",
    "X_selected = X[top_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08f6ddf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Performing PCA...\n",
      "\n",
      "  PCA components: 5\n",
      "  Variance explained:\n",
      "    PC1: 0.8889 (cumulative: 0.8889)\n",
      "    PC2: 0.0697 (cumulative: 0.9586)\n",
      "    PC3: 0.0238 (cumulative: 0.9824)\n",
      "    PC4: 0.0170 (cumulative: 0.9994)\n",
      "    PC5: 0.0006 (cumulative: 1.0000)\n",
      "Saved: 05_pca_variance.png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n  Performing PCA...\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "pca = PCA(n_components=min(N_PCA_COMPONENTS, X_selected.shape[1]), random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "cumsum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "print(f\"\\n  PCA components: {X_pca.shape[1]}\")\n",
    "print(f\"  Variance explained:\")\n",
    "for i in range(min(5, len(pca.explained_variance_ratio_))):\n",
    "    print(f\"    PC{i+1}: {pca.explained_variance_ratio_[i]:.4f} (cumulative: {cumsum_var[i]:.4f})\")\n",
    "\n",
    "# Save PCA results\n",
    "pca_results = pd.DataFrame({\n",
    "    'component': [f'PC{i+1}' for i in range(len(pca.explained_variance_ratio_))],\n",
    "    'explained_variance_ratio': pca.explained_variance_ratio_,\n",
    "    'cumulative_variance': cumsum_var\n",
    "})\n",
    "pca_results.to_csv(os.path.join(TABLES_DIR, 'pca_results.csv'), index=False)\n",
    "\n",
    "# Plot PCA\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "             pca.explained_variance_ratio_, 'o-', linewidth=2)\n",
    "axes[0].set_xlabel('Component', fontsize=11, fontweight='bold')\n",
    "axes[0].set_ylabel('Explained Variance Ratio', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title('Scree Plot', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(range(1, len(cumsum_var) + 1), cumsum_var, 's-', linewidth=2, color='red')\n",
    "axes[1].axhline(0.95, color='k', linestyle='--', alpha=0.5)\n",
    "axes[1].axhline(0.90, color='k', linestyle=':', alpha=0.5)\n",
    "axes[1].set_xlabel('Number of Components', fontsize=11, fontweight='bold')\n",
    "axes[1].set_ylabel('Cumulative Variance', fontsize=11, fontweight='bold')\n",
    "axes[1].set_title('Cumulative Explained Variance', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, '05_pca_variance.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: 05_pca_variance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2223783c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING PROCESSED DATA\n",
      "================================================================================\n",
      "\n",
      "Saved: data_for_modeling_features.csv ((15845, 6))\n",
      "Saved: data_for_modeling_pca.csv ((15845, 6))\n",
      "Saved: feature_scaler.pkl, pca_model.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAVING PROCESSED DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Original selected features\n",
    "df_modeling_features = pd.DataFrame(X_selected, columns=top_features)\n",
    "df_modeling_features['tc_kelvin'] = y.values\n",
    "df_modeling_features.to_csv(os.path.join(OUTPUT_DIR, 'data_for_modeling_features.csv'), index=False)\n",
    "print(f\"\\nSaved: data_for_modeling_features.csv ({df_modeling_features.shape})\")\n",
    "\n",
    "# PCA features\n",
    "df_modeling_pca = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(X_pca.shape[1])])\n",
    "df_modeling_pca['tc_kelvin'] = y.values\n",
    "df_modeling_pca.to_csv(os.path.join(OUTPUT_DIR, 'data_for_modeling_pca.csv'), index=False)\n",
    "print(f\"Saved: data_for_modeling_pca.csv ({df_modeling_pca.shape})\")\n",
    "\n",
    "# Save models\n",
    "import joblib\n",
    "joblib.dump(scaler, os.path.join(MODELS_DIR, 'feature_scaler.pkl'))\n",
    "joblib.dump(pca, os.path.join(MODELS_DIR, 'pca_model.pkl'))\n",
    "print(f\"Saved: feature_scaler.pkl, pca_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3c03b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ANALYSIS COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "DATA SUMMARY:\n",
      "  Total records: 15,845\n",
      "  Original features: 145\n",
      "  After low-variance removal: 145\n",
      "  After multicollinearity removal: 5\n",
      "  Selected top features: 5\n",
      "  PCA components: 5\n",
      "\n",
      "Tc STATISTICS:\n",
      "  Mean: 29.87 K\n",
      "  Median: 14.40 K\n",
      "  Range: 0.01 - 294.00 K\n",
      "  High-Tc (>77K): 2,337 (14.7%)\n",
      "\n",
      "TOP 5 FEATURES (Composite Score):\n",
      "  MagpieData avg_dev GSvolume_pa: 0.799\n",
      "  max ionic char: 0.615\n",
      "  MagpieData range Electronegativity: 0.604\n",
      "  MagpieData range GSvolume_pa: 0.595\n",
      "  MagpieData range CovalentRadius: 0.552\n",
      "\n",
      "PCA VARIANCE:\n",
      "  First 5 components: 100.0% \n",
      "  All 5 components: 100.0%\n",
      "\n",
      "OUTPUT FILES:\n",
      "  Figures: /home/digifort/Documents/Data_Management_F25/supercon/analysis_results/figures/\n",
      "  Tables: /home/digifort/Documents/Data_Management_F25/supercon/analysis_results/tables/\n",
      "  Models: /home/digifort/Documents/Data_Management_F25/supercon/analysis_results/models/\n",
      "  Modeling data: /home/digifort/Documents/Data_Management_F25/supercon/analysis_results/\n",
      "\n",
      "================================================================================\n",
      "Next steps: Run regression models using the prepared datasets!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary = f\"\"\"\n",
    "DATA SUMMARY:\n",
    "  Total records: {len(y):,}\n",
    "  Original features: {len(numerical_features)}\n",
    "  After low-variance removal: {len(X.columns)}\n",
    "  After multicollinearity removal: {len(X_selected.columns)}\n",
    "  Selected top features: {len(top_features)}\n",
    "  PCA components: {X_pca.shape[1]}\n",
    "\n",
    "Tc STATISTICS:\n",
    "  Mean: {y.mean():.2f} K\n",
    "  Median: {y.median():.2f} K\n",
    "  Range: {y.min():.2f} - {y.max():.2f} K\n",
    "  High-Tc (>77K): {(y > 77).sum():,} ({100*(y > 77).sum()/len(y):.1f}%)\n",
    "\n",
    "TOP 5 FEATURES (Composite Score):\n",
    "\"\"\"\n",
    "\n",
    "for i, row in feature_importance.head(5).iterrows():\n",
    "    summary += f\"  {row['feature']}: {row['composite_score']:.3f}\\n\"\n",
    "\n",
    "summary += f\"\"\"\n",
    "PCA VARIANCE:\n",
    "  First 5 components: {cumsum_var[4]:.1%} \n",
    "  All {X_pca.shape[1]} components: {cumsum_var[-1]:.1%}\n",
    "\n",
    "OUTPUT FILES:\n",
    "  Figures: {FIGURES_DIR}\n",
    "  Tables: {TABLES_DIR}\n",
    "  Models: {MODELS_DIR}\n",
    "  Modeling data: {OUTPUT_DIR}\n",
    "\"\"\"\n",
    "#  First 10 components: {cumsum_var[9]:.1%}\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary\n",
    "with open(os.path.join(OUTPUT_DIR, 'ANALYSIS_SUMMARY.txt'), 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Next steps: Run regression models using the prepared datasets!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6421e22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_3.9.13",
   "language": "python",
   "name": "python_3.9.13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
